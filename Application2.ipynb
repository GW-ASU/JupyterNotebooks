{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDaAPZyLFOVQ"
   },
   "source": [
    "# Data Science Application 2:\n",
    "Exploratory Data Analysis and Machine Learning for Bias Diagnosis and Correction of a Groundwater Model\n",
    "\n",
    "The following code snippets demonstrate a typical workflow that analyze errors of groundwater flow model simulation results, detect systematic error (bias), and perform bias correction using\n",
    "machine learning. While the sample dataset used here comes from groundwater flow models, the workflow can be applied to other hydrologic models that generate spatio-temporal data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmy1-wbACx-P"
   },
   "source": [
    "# 0. Install libraries\n",
    "This section installs the `dataretrieval` and `hsclient` libraries from the GitHub repositories. `hsclient` will be used to download/create HydroShare resources, and `dataretrieval` will be used to obtain observational data from USGS NWIS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17364,
     "status": "ok",
     "timestamp": 1695233941974,
     "user": {
      "displayName": "Shiqi Wei",
      "userId": "13817596853751757749"
     },
     "user_tz": 420
    },
    "id": "ciOxVpJQ4myA",
    "outputId": "759cc943-5cf3-4d45-af8e-b4be24892dd9"
   },
   "outputs": [],
   "source": [
    "# # install the dataretrieval package, a Python alternative to USGS-R's dataRetrieval package\n",
    "# !pip install  dataretrieval\n",
    "\n",
    "# # # install the HydroShare RDF Python Client (dataretrieval) package\n",
    "# !pip install hsclient\n",
    "import matplotlib.colors as mcolors\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dataretrieval.nwis as nwis\n",
    "from hsclient import HydroShare\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.linalg import hankel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62JyfeBIhy1X"
   },
   "source": [
    "# 1. Data Retrieval\n",
    "To get started, let's retrieve sample datasets from HydroShare and observational data from USGS NWIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djJNOqRaQ6Rf"
   },
   "source": [
    "## 1.1. Download a sample dataset from a HydroShare resource\n",
    "\n",
    "\n",
    "The Python package `dataretrieval` allows you to download an entire HydroShare resource as a zipped file that uses the BagIt packaging standard. You can identify the resource you want to download using its HydroShare identifier. When you call the `download()` function on the resource, you can pass a path where you want to save the zipped file. Leaving the path blank downloads the files to the directory. For more details on how to use hsclient, refer to https://github.com/hydroshare/hsclient/ and the reference below.\n",
    "\n",
    "For demonstration purpose, we will use a sample dataset containing the groundwater levels simulated by a groundwater flow model at locations where level observations are available from monitoring wells. The specifics are described in the readme.txt in the download and Xu et al. (2015).\n",
    "\n",
    "**References**:  \n",
    "Horsburgh, J. S., S. S. Black (2021). HydroShare Python Client Library (hsclient) Usage Examples, HydroShare, http://www.hydroshare.org/resource/7561aa12fd824ebb8edbee05af19b910\n",
    "\n",
    "Xu, T., Valocchi, A. J., Choi, J., & Amir, E. (2014). Use of machine learning methods to reduce predictive error of groundwater models. Groundwater, 52(3), 448-460."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5zXJJAEocCdS"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  shikita\n",
      "Password for shikita:  ········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/DWSA/7152c722a45b407084954976a8b23aac.zip'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download file from hydro share\n",
    "# Authentication is needed to read from/write to HydroShare resources\n",
    "\n",
    "hs = HydroShare()\n",
    "hs.sign_in()\n",
    "uuid_repository='7152c722a45b407084954976a8b23aac' # the repository uuid you want to download\n",
    "\n",
    "res = hs.resource(uuid_repository)\n",
    "out_path= 'D:/DWSA/' # the folder path where you want to keep the download file\n",
    "res.download(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14Z9-nPNRwXl"
   },
   "source": [
    "## 1.2. Retrieve Observational Data from USGS NWIS\n",
    "USGS National Water Information System (NWIS) is a database containing water resources data collected at over 1 million sites throughout CONUS. The data can be accessed through the NWIS webpage (https://waterdata.usgs.gov/nwis) or via R package `dataRetrieval` (https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html) or  Python counterpart `dataretrieval`. Here, we use the Python package to retrieve groundwater level observations at selected wells. For more information check:\n",
    "https://github.com/USGS-python/dataretrieval.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 1258,
     "status": "error",
     "timestamp": 1666649135527,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "jk4J0jYC4moY",
    "outputId": "a2d586f2-a3b4-46c8-f700-b049b71d002c"
   },
   "outputs": [],
   "source": [
    "# specify the USGS site code for which we want data.\n",
    "# Simplest case: one groundwater observation well\n",
    "site='410618098113401' # site number (water_id)\n",
    "start_time='2007-10-01' # specify the desired start time from when data will be retrieved\n",
    "end_time='2022-08-23'# specify ending time\n",
    "\n",
    "# the get_record function retrieves data, option are: instantaneous values (iv), daily value (dv), statistics (stat),site info (site),discharge peaks (peaks),discharge measurements (measurements)\n",
    "df_daily_value = nwis.get_record(sites=site, service='dv', start=start_time, end=end_time) # example of get daily water level value in time range (star, end)\n",
    "# site info contains site characteritics such as land surface elevation and datum (for groundwater levels)\n",
    "df_site_info = nwis.get_record(sites=site, service='site')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZJXbA_MnDPC"
   },
   "source": [
    "Data can also be downloaded in a batch mode for multiple side numbers. In the example below, we use a subset of the wells included in the `water_level_example.csv` file contained in the downloaded zip file to create a list of wells for data retrieval from NWIS. Only a subset is used so that the runtime is kept short. In a more general case, one could also specify a study area to obtain information of si within that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 11034,
     "status": "ok",
     "timestamp": 1666649184867,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "fyYAx0QYdMHY",
    "outputId": "38e113c7-11fc-4270-b32d-4946959ee513"
   },
   "outputs": [],
   "source": [
    "# This is an example of how to get the 'depth to water' of a list of wells.\n",
    "\n",
    "file_path='D:/DWSA/water_level_example.xlsx'  # specify directory of this file\n",
    "df=pd.read_excel(file_path)\n",
    "# use the following commands when using Google Colab (may need to be tweaked for other online platforms)\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv(io.BytesIO(uploaded[file_path]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_no</th>\n",
       "      <th>x_coordinates</th>\n",
       "      <th>y_coordinates</th>\n",
       "      <th>date</th>\n",
       "      <th>MODFLOW</th>\n",
       "      <th>residual</th>\n",
       "      <th>weights</th>\n",
       "      <th>date_formated</th>\n",
       "      <th>well_id</th>\n",
       "      <th>observation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>828290</td>\n",
       "      <td>266030</td>\n",
       "      <td>5128</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>-2.169</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>2004-09-14</td>\n",
       "      <td>474047117280601</td>\n",
       "      <td>1668.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>829600</td>\n",
       "      <td>258060</td>\n",
       "      <td>5072</td>\n",
       "      <td>1696.6</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.1886</td>\n",
       "      <td>2004-07-20</td>\n",
       "      <td>473928117275001</td>\n",
       "      <td>1697.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>829600</td>\n",
       "      <td>258060</td>\n",
       "      <td>5102</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>473928117275001</td>\n",
       "      <td>1694.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>829600</td>\n",
       "      <td>258060</td>\n",
       "      <td>5129</td>\n",
       "      <td>1694.5</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>2004-09-15</td>\n",
       "      <td>473928117275001</td>\n",
       "      <td>1695.551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   well_no  x_coordinates  y_coordinates  date  MODFLOW  residual  weights  \\\n",
       "0        1         828290         266030  5128   1671.0    -2.169   0.1960   \n",
       "1        2         829600         258060  5072   1696.6     0.578   0.1886   \n",
       "2        2         829600         258060  5102   1695.0    -0.200   0.1922   \n",
       "3        2         829600         258060  5129   1694.5     1.051   0.1922   \n",
       "\n",
       "  date_formated          well_id  observation  \n",
       "0    2004-09-14  474047117280601     1668.831  \n",
       "1    2004-07-20  473928117275001     1697.178  \n",
       "2    2004-08-19  473928117275001     1694.800  \n",
       "3    2004-09-15  473928117275001     1695.551  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "error",
     "timestamp": 1666649188732,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "d5RpX5ydg1uY",
    "outputId": "f9520b99-4d9f-4d1f-c077-caa3c8dbf104"
   },
   "outputs": [],
   "source": [
    "# get the groundwater level of  insitu USGS water level of well and date in df\n",
    "demo_dataset = df.iloc[0:4,:] # use first 5 wells for demo. use the colume \"well_id\" to query NWIS, which are well IDs following USGS naming convention\n",
    "\n",
    "# 1: for continuous measurements use option 'dv' to get daily mean or 'iv' for instantaneous value to get groundwater level, for example:\n",
    "# df_demo = [nwis.get_record(sites=str(demo_dataset['well_id'][st]), service='dv', start=demo_dataset['date_formated'][st], end=demo_dataset['date_formated'][st]) for st in range (len(demo_dataset['well_id']))] # example of get daily water level for continuous measurements\n",
    "\n",
    "# 2: for historic and discontinuous measurements, call 'site' and get the 'well_depth_va' to get depth to water and land surface altitude\n",
    "# obtaining data of 'depth to groundwater table' in ft.\n",
    "df_demo = [nwis.get_record(sites=str(demo_dataset['well_id'][st]), service='site', start=demo_dataset['date_formated'][st], end=demo_dataset['date_formated'][st])['well_depth_va'].item() for st in range (len(demo_dataset['well_id']))]\n",
    "\n",
    "# get land surface altitude of the site. Detailed parameter description can be found : http://usgs-r.github.io/dataRetrieval/reference/readNWISsite.html\n",
    "surface_altitude_demo = [nwis.get_record(sites=str(demo_dataset['well_id'][st]), service='site')['alt_va'].tolist() for st in range (len(demo_dataset['well_id']))]\n",
    "\n",
    "# 'groundwater level' = 'surface altitude' - 'depth to water table'\n",
    "water_level_observations_demo = np.array(surface_altitude_demo).flatten()-np.array(df_demo).flatten()\n",
    "\n",
    "# add insitu observation to dataframe\n",
    "demo_dataset['isnitu_water_level'] = water_level_observations_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJgw2Og3k1gT"
   },
   "source": [
    "## 1.3 Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Umefp4MloBrl"
   },
   "source": [
    "#### This section is about projecting the model output to synchronize with USGS data, which is usually in latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "error",
     "timestamp": 1695235873477,
     "user": {
      "displayName": "Shiqi Wei",
      "userId": "13817596853751757749"
     },
     "user_tz": 420
    },
    "id": "2FJB-CaYoAwm",
    "outputId": "42571981-ffff-49e6-cdd0-475a76dd1468"
   },
   "outputs": [],
   "source": [
    "# This is an example of how to import the shp of model\n",
    "shp_file_path='D:/DWSA/gis-colab/gis/1701_wb_shp.shp'#1701_area_shp#1701_wb_shp#active_cell_layer_3#svrp_model_boundary\n",
    "# The \n",
    "gdf = gpd.GeoDataFrame(demo_dataset, geometry=gpd.points_from_xy(demo_dataset.x_coordinates, demo_dataset.y_coordinates))\n",
    "\n",
    "# read the shapefile\n",
    "df_shp = gpd.read_file(shp_file_path)\n",
    "geo_shp = df_shp['geometry']\n",
    "\n",
    "# Convert the polygon to a WKT string\n",
    "geo_gdf = gpd.GeoDataFrame(geometry=geo_shp)\n",
    "\n",
    "# Define the geographic coordinate system (replace XXXX with your desired GCS)\n",
    "crs_latlon = pyproj.CRS.from_epsg(XXXX)\n",
    "\n",
    "# Convert the polygon coordinates to geographic coordinates\n",
    "geo_gdf2 = geo_gdf.to_crs(crs_latlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E50QTBmlJT3V"
   },
   "source": [
    "# 2. Error Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wkNHDETFHHg"
   },
   "source": [
    "## 2.1. Compute Residual\n",
    "Residual is defined as observation minus model simulation results at the time and location of the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j1-9MWQUT7k"
   },
   "outputs": [],
   "source": [
    "# y_mod : the data computed hydrology model\n",
    "# y_insitu: the data from insitu observation\n",
    "def computer_residual(y_mod, y_insitu):\n",
    "    res = y_insitu - y_mod\n",
    "    return(res)\n",
    "\n",
    "y_mod = df['MODFLOW']\n",
    "y_insitu = df['observation']\n",
    "y_res = y_insitu - y_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fcqvKCJRyVF"
   },
   "source": [
    "## 2.2. Performance metrics\n",
    "Here we include four commonly used performance metrics as defined below:\n",
    "1. PBIAS: Percentage Bias (Gupta et al., 1999)\n",
    "2. RMSE: Root Mean Sqaure Error;\n",
    "3. NSE: Nash–Sutcliffe Efficiency;\n",
    "4. KGE: Sutcliffe and Kling-Gupta Efficiency (Gupta et al., 2009)\n",
    "\n",
    "NSE and KGE are best suited for time series data, for example, streamflow observations at one gaging station, and monthly groundwater level at a monitoring well\n",
    "\n",
    "**References**\n",
    "\n",
    "Gupta, H.V.; Sorooshian, S.; Yapo, P.O. Status of Automatic Calibration for Hydrologic Models: Comparison with Multilevel Expert Calibration. Journal of Hydrologic Engineering 1999, 4, 135–143, doi:10.1061/(ASCE)1084-0699(1999)4:2(135).\n",
    "\n",
    "Gupta, H.V., Kling, H., Yilmaz, K.K. and Martinez, G.F., 2009. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2), pp.80-91.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwNrSSeTR3V_"
   },
   "outputs": [],
   "source": [
    "def PBIAS(y_insitu, y_mod):\n",
    "    a=np.sum(y_insitu-y_mod)\n",
    "    b=np.sum(y_insitu)\n",
    "    return (a/b)\n",
    "\n",
    "\n",
    "def RMSE(y_insitu, y_mod):\n",
    "    return(np.sqrt(np.mean((y_insitu-y_mod)**2)))\n",
    "\n",
    "\n",
    "def NSE(y_insitu, y_mod):\n",
    "    y_mean=y_insitu.mean()\n",
    "    rss=np.sum((y_insitu-y_mod)**2)\n",
    "    tss=np.sum((y_insitu-y_mean)**2)\n",
    "    return (1-rss/tss)\n",
    "\n",
    "\n",
    "def KGE(y_insitu, y_mod):\n",
    "    variability_error=(np.std(y_insitu)/np.std(y_mod) -1)**2\n",
    "    bia_error=(np.mean(y_insitu)/np.mean(y_mod)-1)**2\n",
    "    linear_corre=(np.corrcoef(y_insitu,y_mod)[1,0] - 1)**2\n",
    "    return(1-np.sqrt(variability_error+bia_error+linear_corre))\n",
    "\n",
    "\n",
    "# Here we calculate NSE and KGE for individual sites having 10 or more\n",
    "# observations, and then take the average across sites\n",
    "def kge_nse(df, a,b):\n",
    "# df is dataset, a,b are variables' name (in string) in df that you want to calculate KGE and NSE\n",
    "    well_name=df['well_no']\n",
    "    group_by=[df[well_name==i] for i in set(well_name)] # group  dataset by each site\n",
    "    kge_per_well=[KGE(k[a].to_numpy(), k[b].to_numpy()) if len(k)>=10 else np.nan for k in group_by] # calculate KGE for sites have at least 10 observations\n",
    "    nse_per_well=np.array([NSE(k[a].to_numpy(), k[b].to_numpy()) if len(k)>=10 else np.nan  for k in group_by]) # calculate NSE for have at least 10 observations\n",
    "    return(kge_per_well,nse_per_well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pGkzNzPhy1f"
   },
   "source": [
    "Below, we calculate the above metrics using the sample data or your own data. For the sample data, we found an overall small bias but substantial RMSE. The NSE and KGE scores show a wide range of variability across observation wells, with some wells having very low NSE and KGE scores, suggesting gap between simulation results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1666649200434,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "2aR7nYbK2CwC",
    "outputId": "05bc76ed-6866-46f9-d565-5c25afd83398"
   },
   "outputs": [],
   "source": [
    "well_name = df['well_no']\n",
    "\n",
    "kge_per_well,nse_per_well = kge_nse(df, 'observation','MODFLOW')\n",
    "nse_rm_inf_nan = nse_per_well[np.isfinite(nse_per_well)] # remove Inf value\n",
    "\n",
    "print('pbias', PBIAS(df['observation'],df['MODFLOW']))\n",
    "print('rmse', RMSE(y_insitu,y_mod))\n",
    "print('nse', np.nanmedian(nse_rm_inf_nan))\n",
    "print('kge', np.nanmedian(kge_per_well))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCXl0T1CR2NT"
   },
   "source": [
    "## 2.3. Residual plots\n",
    "Various types of residual plots can be used to describe the statistical attributes of errors in more detail than the performance metrics above. These residual plots can reveal \"structures\" in the model residual, such as spatial and temporal correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1jZJcUpjDxr"
   },
   "source": [
    "Scatter plot shows residual vs. observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1666649203812,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "dldIImcxjZBc",
    "outputId": "d3b71b86-b09f-447b-dd72-bae4589cccc3"
   },
   "outputs": [],
   "source": [
    "date = df['date_formated']\n",
    "fig,axes=plt.subplots(nrows=1, ncols=1, sharex='all',figsize=(5,5),dpi=300)\n",
    "plt.scatter(df['observation'],df['residual'], marker='*',c='red' ,s=1)\n",
    "plt.xlabel('Observation (ft)')\n",
    "plt.ylabel('Residual (ft)')\n",
    "plt.title('Scatter plot of residual vs observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXeZUb_XjqNk"
   },
   "source": [
    "Time series plots at an individual site can be used to examine how well the model simulation replicates the observed dynamics at different locations of the study area. In the example below, we plot the observed and simulated groundwater level at two wells at which the model is behaving very well with the highest KGEs. It can be seen that the model overall captures the multi-year trend and seasonal fluctuation of water level, however seems to underestimate the peaks in summer and late fall/winter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 1451,
     "status": "ok",
     "timestamp": 1666649208747,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "moCTm8Zxjoql",
    "outputId": "1c89f02c-4978-4c1c-a650-06a5238f04f3"
   },
   "outputs": [],
   "source": [
    "\"'1: Let's select 2 wells with highest KGE value'\"\n",
    "\n",
    "# combine NSE and KGE to dataframe for easy observation\n",
    "nse_kge_dataframe=pd.DataFrame(list(zip(kge_per_well, nse_per_well,set(well_name))),  columns=['NSE','KGE', 'well_no'])\n",
    "nse_kge_sort=nse_kge_dataframe.sort_values(by='NSE', ascending=False) # rank discending base on NSE value. Can also replace NSE by KGE.\n",
    "\n",
    "# select top 2 wells\n",
    "well_1_no=nse_kge_sort.iloc[1]['well_no'].astype('int')\n",
    "well_2_no=nse_kge_sort.iloc[2]['well_no'].astype('int')\n",
    "\n",
    "well_2=df[df['well_no']==well_2_no]\n",
    "well_1=df[df['well_no']==well_1_no]\n",
    "\n",
    "x_index_1=pd.to_datetime(well_1['date_formated'])\n",
    "x_index_2=pd.to_datetime(well_2['date_formated'])\n",
    "\n",
    "\"'2: Let's plot selected wells time series'\"\n",
    "import matplotlib.dates as mdate\n",
    "\n",
    "fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(10,5),dpi=300)\n",
    "axes[0].plot(x_index_1, well_1['observation'],'o-',label='Well No. '+str(well_1_no)+' observation')\n",
    "axes[0].plot(x_index_1, well_1['MODFLOW'],'*-',label='Well No. '+str(well_1_no)+' MODFLOW')\n",
    "axes[1].plot(x_index_2, well_2['observation'],'o-',label='Well No. '+str(well_2_no)+' observation')\n",
    "axes[1].plot(x_index_2, well_2['MODFLOW'],'*-',label='Well No. '+str(well_2_no)+' MODFLOW')\n",
    "\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Water level (ft)')\n",
    "# axes.set_title('Time series plot')\n",
    "axes[0].tick_params(direction='in')\n",
    "axes[0].set_xticklabels(x_index_1,rotation=90)\n",
    "monthFmt = mdate.DateFormatter('%Y-%m')\n",
    "axes[0].xaxis.set_major_formatter(monthFmt)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Water level (ft)')\n",
    "# axes.set_title('Time series plot')\n",
    "axes[1].tick_params(direction='in')\n",
    "axes[1].set_xticklabels(x_index_1,rotation=90)\n",
    "monthFmt = mdate.DateFormatter('%Y-%m')\n",
    "axes[1].xaxis.set_major_formatter(monthFmt)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCAwZAwJjhm8"
   },
   "source": [
    "Plotting time-averaged residuals in a map shows spatial trends of residuals and can reveal locations where the model is systematically over- or under- predicting. In the example below, large residuals mostly occur near the model boundary and near the Spokane River."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1666649230648,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "Xfm4IVtPjm61",
    "outputId": "977c3966-e4df-419e-a2dc-caaf3fbf8f26"
   },
   "outputs": [],
   "source": [
    "df_time_average=df.groupby(df['well_no']).mean() # get the temporal average value of each site\n",
    "\n",
    "x_time_average=df_time_average['x_coordinates']\n",
    "y_time_average=df_time_average['y_coordinates']\n",
    "res_time_average=df_time_average['residual']\n",
    "fig,axes=plt.subplots(nrows=1, ncols=1, sharex='all',figsize=(5,5),dpi=300)\n",
    "\n",
    "offset = mcolors.TwoSlopeNorm(vmin=res_time_average.min(),\n",
    "                                  vcenter=0, vmax=res_time_average.max())\n",
    "\n",
    "plt.scatter(x_time_average, y_time_average, c=res_time_average, norm=offset,cmap='RdBu')\n",
    "plt.title(\"Point observations\")\n",
    "plt.xlabel(\"x (mi.)\")\n",
    "plt.ylabel(\"y (mi.)\")\n",
    "cbar= plt.colorbar()\n",
    "cbar.set_label('Residual (ft)', labelpad=+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN6VOIuvkOe3"
   },
   "source": [
    "Finally, the Q-Q (quantile-quantile) plot can be used to test whether the error is white noise, i.e., i.i.d. normal with a zero mean. A Q-Q plot graphically compares the distribution of the residual with a reference distribution (a normal distribution is used here). If the residual follows a normal distribution, the plots in the Q-Q plot will approximately lie on the identity line (y=x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTg6LcL7SDxY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def qqplot(x):\n",
    "    fig, ax = plt.subplots(ncols=1,nrows=1, figsize=(5,5),dpi=300)\n",
    "\n",
    "    sm.qqplot(x, fit=True, line ='45', ax=ax)\n",
    "    ax.grid()\n",
    "    ax.set_title('Q-Q plot of the residual',fontsize=15)\n",
    "    ax.xaxis.get_label().set_fontsize(12)\n",
    "    ax.yaxis.get_label().set_fontsize(12)\n",
    "    ax.get_lines()[1].set_color(\"red\")\n",
    "    ax.get_lines()[1].set_linewidth(\"2\")\n",
    "    plt.show()\n",
    "    return(fig, plt.show())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqx0LmnZVcZw"
   },
   "source": [
    "Using the sample dataset, the Q-Q plot overall appears steeper than the line y=x, indicating the residual distribution is more dispersed than a normal distribution. In addition, the Q-Q plot is \"S\" shaped, suggesting the residual distribution is skewed and heavy tailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1666649230959,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "EKcQiQJI5BRp",
    "outputId": "836ca1a5-552f-41a6-c142-d5d87e229d38"
   },
   "outputs": [],
   "source": [
    "qqplot(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJQWX7E6SFwN"
   },
   "source": [
    "## 2.4. Correlation analyses\n",
    "Next, we will calculate and visualize some statistics regarding the correlation structures of the residual. Specifically, we examine autocorrelation and spatial correlation as below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tFdMTm4k2Mo"
   },
   "source": [
    "ACF, or autocorrelation function, characterize the correlation of a time series with a delayed copy of itself as a function of delay. ACF is often used to find periodic (e.g., seasonal, diurnal) patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ykx76VwLa70"
   },
   "outputs": [],
   "source": [
    "def ACF(x, lag): # lag should <= length of x\n",
    "    if lag <= len(x):\n",
    "        n=len(x)\n",
    "        return (np.corrcoef(x[:n-lag], x[lag:])[1,0])\n",
    "    else:\n",
    "        return(print('lag should <= length of x, in this case it is '+ str(len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEwd0DhGGLCF"
   },
   "outputs": [],
   "source": [
    "def plot_acf(well_acf_no,df,variable_name, lag): #well_acf_no:  well_no you want to plot, lag should <= length of x\n",
    "    well_acf=df[df['well_no']==well_acf_no]\n",
    "    ACF(well_acf[variable_name],lag)\n",
    "    acf_plot=[ACF(well_acf[variable_name],i) for i in range (lag)]\n",
    "    \"'selected well acf plot'\"\n",
    "    plt.plot(list(range(lag)), acf_plot, '*-')\n",
    "    plt.xlabel('Lags (Months)')\n",
    "    plt.ylabel('Correlation coefficient')\n",
    "    plt.title('ACF of well ' +str(well_acf_no) )\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1666649231708,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "6yJ4rM1SkRuV",
    "outputId": "d40fd689-5d47-499a-e0b5-78fefc81634f"
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(ncols=1,nrows=1,figsize=(5,5), dpi=300)\n",
    "plot_acf(92,df,'residual',40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjkuSTWMlEPI"
   },
   "source": [
    "We use variogram to analyze the spatial correlation structure of residuals. For more information about variogram, please refer to [here](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/variogram#:~:text=A%20variogram%20is%20defined%20as,Comprehensive%20Geographic%20Information%20Systems%2C%202018).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dysRQcFjlCso"
   },
   "outputs": [],
   "source": [
    "def dist(x, y, res): # here lon, lat, res, should be in numpy array or list\n",
    "    diff_y=[(k-v)**2/2 for k in res for v in res]\n",
    "    diff_dis=[ ((x[i] -x[j] )**2 + (y[i]-y[j])**2)**0.5 for i in range(len(x)) for j in range(len(y))] # Euclidean distance\n",
    "    return (diff_y, diff_dis)\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "def kg_plot(diff_y, diff_dis,n_bin,res_time_average):\n",
    "    x_plot=np.linspace(start=min(diff_dis), stop=max(diff_dis), num=n_bin)\n",
    "    d={'dis': diff_dis, 'res': diff_y}\n",
    "    new_df=pd.DataFrame(data=d)\n",
    "    plt_y=new_df['res'].groupby(pd.cut(new_df[\"dis\"], x_plot)).mean()\n",
    "    fig, ax=plt.subplots(ncols=1,nrows=1,figsize=(5,5), dpi=300)\n",
    "    plt.plot(x_plot[:-1]/5280,plt_y,'s-',label='Residual semi-variogram')\n",
    "    plt.plot(x_plot[:-1]/5280,[statistics.variance(res_time_average) for k in range (len(x_plot[:-1]))], '--', label='Residual variance')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Distance (mi.)')\n",
    "    plt.ylabel(r'$\\gamma (ft^2)$')\n",
    "    plt.title('Variogram')\n",
    "    return (fig, plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJe3R3FmsUMa"
   },
   "source": [
    "We plot the semi-variogram of the time average of residuals at all wells. In the example below, the semi-variogram increases with distance, suggesting spatial correlation exists. This is consistent with the spatial patterns observed in the spatial plot of time average of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1666649232729,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "YnQmjW_x5uwB",
    "outputId": "7ffeb1b4-57ea-4e4b-8e07-df7eb92e6e3b"
   },
   "outputs": [],
   "source": [
    "diff_y, diff_dis = dist(x_time_average.to_numpy(), y_time_average.to_numpy(), res_time_average.to_numpy())\n",
    "n_bin = 30\n",
    "kg_plot(diff_y, diff_dis,n_bin,res_time_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUgmXOJjRJ5b"
   },
   "source": [
    "# 3. Bias Correction using Machine Learning\n",
    "\n",
    "The residual analyses and plots performed on the sample dataset revealed that the groundwater model simulation results contain systematic error (or bias) even though the model has been calibrated to overall good accuracy. Bias may come from inevitable simplifications when we build numerical models of the complicated reality, model parameters not fully representing the heterogeneity, or errors in the input data. These sources of biases are difficult to address using a physically based groundwater model. In this case, machine learning can be used to correct these biases.\n",
    "\n",
    "In this demo, we will build machine learning models to characterize the function between residual (y) and explanatory variables (x). We can then use the machine learning predicted residual to bias correct simulation results of groundwater models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KitocafISXmj"
   },
   "source": [
    "## 3.1. Feature engineering\n",
    "Conventional machine learning algorithms would often benefit from feature engineering guided by expert knowledge, particularly when a large number of explanatory variables are present and are potentially related to the target variable. Feature engineering is the process of selecting input variables, sometimes involving transforming the input variables. Here we use mutual information and multivariate singular spectrum analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUbjyoHDXKs5"
   },
   "source": [
    "### Mutual Information\n",
    "\n",
    "For two variables, mutual information is the score that describle in what level one variable can explain the other. This example use the `normalized_mutual_info_score` function in `scikit-learn` to calculate the normalized version of mutual information, i.e., in the range of (0,1), with 0 suggesting no relation between the two variables.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Learned-Miller, E.G., 2013. Entropy and mutual information. Department of Computer Science, University of Massachusetts, Amherst, p.4. access : https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa7b-d0FC0lP"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html\n",
    "\n",
    "#kk=normalized_mutual_info_score(x, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0DVNYsPnemu"
   },
   "source": [
    "### Multivariate Singular Spectrum Analysis (MSSA)\n",
    "\n",
    "Here, we provide a brief, intuitive description of MSSA. More technical details can be found in the references below. Singular Spectrum Analysis (SSA) is an extension of the classical principal component analysis. Given a time series, SSA computes a lag-covariance matrix and its eigenvalues and eigenvectors (empirical orghogonal functions, or EOFs). It then decompose the time series into temporal principle components by projecting the time series onto each EOF. MSSA is an extension of SSA to multivariate time series by calculating the lag-covariance matrix across the variables.\n",
    "\n",
    "**References**\n",
    "\n",
    "Hassani, H. and Mahmoudvand, R., 2013. Multivariate singular spectrum analysis: A general view and new vector forecasting approach. International Journal of Energy and Statistics, 1(01), pp.55-83.\n",
    "\n",
    "Singular-Spectrum Analysis: SSA notebook: https://www.kaggle.com/code/jdarcy/introducing-ssa-for-time-series-decomposition/notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYB4xYgMnWG2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# normalise input variables\n",
    "def normalization_df(df):\n",
    "    normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "    return(normalized_df)\n",
    "\n",
    "\n",
    "# embeding - form the trajectory matrix with lagged copies of the time series\n",
    "def svd(trajmat):\n",
    "\n",
    "    u, s, vh = np.linalg.svd(trajmat)\n",
    "    d = np.linalg.matrix_rank(trajmat)\n",
    "    X_elem = np.array( [s[i] * np.outer(u[:,i],vh[i,:]) for i in range(d)] )\n",
    "    return(X_elem)\n",
    "\n",
    "\n",
    "# diagonal averaging\n",
    "def reconstruction(X_i):\n",
    "    # Reverse for anti diagonal\n",
    "    X_rev =np.flipud(X_i)\n",
    "    # Full credit to Mark Tolonen at https://stackoverflow.com/a/6313414 for this one:\n",
    "    return(np.array([X_rev.diagonal(i).mean() for i in range(-X_i.shape[0]+1, X_i.shape[1])]))\n",
    "\n",
    "\n",
    "# Reconstruct time series and plot\n",
    "def rcons_plot(noiseless_level,X_elem,d, df_plot): # d is rank of embedding mat\n",
    "\n",
    "    n = min(noiseless_level,d) # In case of noiseless time series with d < 12.\n",
    "\n",
    "    # Convert elementary matrices straight to a time series\n",
    "    start=0\n",
    "    fig,axes=plt.subplots(4,1, sharex='all',sharey='all',figsize=(10,10),dpi=300)\n",
    "    axes = np.array(axes).flatten()\n",
    "    for v in range(4):\n",
    "\n",
    "        end=(v+1)*389\n",
    "        for i in range(n):\n",
    "            hhh=X_elem[i][:,start:end]\n",
    "            F_i = reconstruction(hhh)\n",
    "            x_plot=list(range(len(F_i)))\n",
    "            axes[v].plot(x_plot, F_i, lw=2)\n",
    "        start=end-1\n",
    "        # x_plot_2=list(range(len(df_plot.iloc[:,v])))\n",
    "        axes[v].plot(list(range(len(df_plot.iloc[:,v]))), df_plot.iloc[:,v], alpha=1, lw=1)\n",
    "\n",
    "\n",
    "        axes[v].set_ylabel(r\"$\\tilde{F}_i(t)$\")\n",
    "        legend = [r\"$\\tilde{F}_{%s}$\" %i for i in range(n)] + [\"$F$\"]\n",
    "        axes[v].set_title(\"The First \" + str(n) + \" Components of \" + df_plot.iloc[:,v].name)\n",
    "        plt.legend(legend, loc=(1.05,0.1));\n",
    "        if v>2:\n",
    "            axes[v].set_xlabel(\"$t$\")\n",
    "    return()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzFYf0g5LSWX"
   },
   "source": [
    "### Example\n",
    "To illustrate the usage of MSSA, we use a second sample dataset from Xu et al. (2015). The dataset includes the residual of baseflow (referring to groundwater discharge to rivers) simulated by a groundwater flow model when compared to baseflow estimations determined from hydrograph separation. The sample dataset contains several variables that are potentially related to the baseflow residual, including:\n",
    "\n",
    "*   River segment id\n",
    "*   Precipitation in the vicinity of river segment\n",
    "*   Potential evapotranspiration (PET) in the vicinity of river segment\n",
    "*   Estimated irrigation amount in the vicinity of river segment\n",
    "*   Model computed groundwater levelin the vicinity of river segment\n",
    "*   Model computed baseflow\n",
    "\n",
    "Here, we will use MSSA to determine the degree of relevancy of these variables to baseflow residual.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Xu, T., & Valocchi, A. J. (2015). Data-driven methods to improve baseflow prediction of a regional groundwater model. Computers & Geosciences, 85, 124-136."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 16623,
     "status": "ok",
     "timestamp": 1666649249348,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "PkBgJGuaFqur",
    "outputId": "4579d7b0-5557-488d-c451-36057247dd69"
   },
   "outputs": [],
   "source": [
    "# If running Jupyter notebook locally\n",
    "file_path_2 ='discharge_example.csv' # included in the downloaded HydroShare resource\n",
    "#df_2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# If using Google Colab (may need to be tweaked for other online platforms)\n",
    "uploaded = files.upload()\n",
    "df_2 = pd.read_csv(io.BytesIO(uploaded[file_path_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1666649249956,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "-zaoR4mMQoUV",
    "outputId": "c04f61d5-3dd2-479e-b2d7-5bfc61e88724"
   },
   "outputs": [],
   "source": [
    "\"'Exmaple of mutual information score'\"\n",
    "# import sys\n",
    "# print(sys.setrecursionlimit(2000))\n",
    "df_2.columns\n",
    "mi_input=df_2.columns[:-1]\n",
    "mi_val=mutual_info_regression(df_2[mi_input],df_2['res'])\n",
    "fig, ax=plt.subplots(ncols=1,nrows=1,figsize=(5,5), dpi=300)\n",
    "plt.bar(list(range( len(mi_val))),mi_val,label='Mutual information score')\n",
    "plt.xticks(list(range( len(mi_val))),mi_input)\n",
    "plt.legend()\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Mutual information score')\n",
    "plt.title('Mutual information')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "executionInfo": {
     "elapsed": 3816,
     "status": "ok",
     "timestamp": 1666649253770,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "tB9Zf-cOS-Sm",
    "outputId": "9f79d91c-751e-44c6-8f00-1bef1c52a7e3"
   },
   "outputs": [],
   "source": [
    "\"'Exmaple of MSSA'\"\n",
    "# first, normalize data\n",
    "df_2_norm = normalization_df(df_2)\n",
    "# row of trajectory (embedded) matrix\n",
    "window = 100\n",
    "N = len(df_2_norm['irr'])\n",
    "# length.\n",
    "K = N - window+ 1 # The number of columns in the trajectory matrix.\n",
    "# embedding variable to mat\n",
    "trajmat_1 = hankel(df_2_norm['irr'][0:window], r=df_2_norm['irr'][window:])\n",
    "trajmat_2 = hankel(df_2_norm['Qcomp'][0:window], r=df_2_norm['Qcomp'][window:])\n",
    "trajmat_3 = hankel(df_2_norm['ppt'][0:window], r=df_2_norm['ppt'][window:])\n",
    "trajmat_4 = hankel(df_2_norm['evt'][0:window], r=df_2_norm['evt'][window:])\n",
    "trajmat = np.concatenate((trajmat_2,trajmat_1, trajmat_4,trajmat_3), axis=1)\n",
    "# decompose\n",
    "d = np.linalg.matrix_rank(trajmat)\n",
    "X_elem = svd(trajmat)\n",
    "\n",
    "X_name = ['irr','Qcomp','ppt','evt']\n",
    "df_plot = df_2_norm[X_name]\n",
    "rcons_plot(7,X_elem,d, df_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXaDJ1oZSqrr"
   },
   "source": [
    "# 3.2. Data preprocessing\n",
    "\n",
    "Before using machine learning for bias correction, it's a good idea pre-process data for numerical stability. Two commonly used linear scaling practices are:\n",
    "(1) Standardization, which scales data to have a mean of 0 and standard deviation of 1.\n",
    "(2) Normalization, which scales data into a range of `[0,1]`.\n",
    "In general, normalization works better if the dataset has outliers. Note that in cases where data distribution is very skewed and/or tailed, it may be desirable to perform nonlinear transformation to make data more \"normally\" distributed and easier for machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u-ltaKORG6F"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Standardization\n",
    "def Standardization_df(df):\n",
    "    std_df=(df-df.mean())/df.std()\n",
    "    return(std_df)\n",
    "\n",
    "\n",
    "# Normalization\n",
    "def normalization_df(df):\n",
    "    normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "    return(normalized_df)\n",
    "\n",
    "def scale_back(df_norm, df):\n",
    "    scale_back_df=df_norm*(df.max()-df.min()) +df.min()\n",
    "    return(scale_back_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65wynj4_95PI"
   },
   "outputs": [],
   "source": [
    "X_name = ['x_coordinates','y_coordinates','date','MODFLOW']\n",
    "df_y = normalization_df(df['residual'])\n",
    "# df_y = df['residual']\n",
    "# in this example we choose to normalize x (explanatory) variables\n",
    "df_X = normalization_df(df[X_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2xvP-p5PEkX"
   },
   "source": [
    "Next, we will randomly split the dataset into training and test portions. We will use the training set to train the machine learning models and test to assess the generalization performance of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1666649253771,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "ti5R-ClePDeh",
    "outputId": "77448dd1-b594-42b1-c1ae-e49ab1214135"
   },
   "outputs": [],
   "source": [
    "# for reproducibility, we set a random seed here. You can use arbitrary value\n",
    "random_seed = 117\n",
    "# fraction of test data\n",
    "split_fraction = 0.2\n",
    "# Splitting into training, validation, and test subsets\n",
    "X_tr,  X_te, y_tr, y_te, df_tr,df_te=train_test_split(df_X,df_y,df,test_size=split_fraction, random_state=random_seed)\n",
    "# check the size\n",
    "X_tr.shape, y_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em1hYaDsUXvu"
   },
   "source": [
    "# 3.3. Gaussian Process Regression\n",
    "\n",
    "Gaussian process regression (GPR) is a Bayesian kernel regression method that uses a Gaussian Process (GP) to describe the distribution of a variable and the Bayes' theorem to infer the posterior distribution conditioned on data. An advantage of GPR is that it is inherently Bayesian and thus can provide uncertainty estimates associated with prediction. For technical details, plese refer to Williams and Rasmussen (2006). Here, we use the `sklearn.gaussian_process` package.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Williams, C. K., & Rasmussen, C. E. (2006). Gaussian processes for machine learning (Vol. 2, No. 3, p. 4). Cambridge, MA: MIT press.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK0PnSpliYEC"
   },
   "source": [
    "A GP is fully specified by a mean function and a covariance function (also known as kernel). Before seeing any of the data, we have the *a priori* that the residual should have a zero mean and a squared exponential kernel, which is commonly used as the first choice in various applications. The kernel has two hyperparameters, namely *range* and *sill*, that control the correlation scale (similar to the range in the variogram) and signal variance, respectively. With training data, the two hyperparameters can be determined by maximizing likelihood. Using the Bayes' theorem, GPR let the observational data to \"sculpt\" the prior into posterior, which can be evaluated at unseen points to generate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zbcTBoxUrja"
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# use other kernel as needed.\n",
    "# useful link of avaliable kernel and instruction:\n",
    "# https://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "\n",
    "def gpr_pred(X_train, y_train,X_te, Kernel):\n",
    "  # hyperparameters are determined during training\n",
    "    gpr = GaussianProcessRegressor(kernel=Kernel,\n",
    "        random_state=random_seed).fit(X_train, y_train)\n",
    "    # making predictions\n",
    "    y_pred = gpr.predict(X_te)\n",
    "    print('Testing score:')\n",
    "    print(gpr.score(X_te, y_pred))\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqUWOUtdgyVa"
   },
   "source": [
    "Running the defined `gpr` regressor will output GPR predicted residual at test points. We will evaluate the accuracy later.\n",
    "\n",
    "The length_scale determines the length of the 'wiggles' in your function. In general, you won't be able to extrapolate more than this number away from your data. The smaller the number the complex the model, thus more likely to overfit.\n",
    "\n",
    "The parameter in front of kernel is  variance σ2; determines the average distance of your function away from its mean. it is actually a scale factor.\n",
    "\n",
    "The parameter add to kernel is offset; the mean of distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27756,
     "status": "ok",
     "timestamp": 1666649281515,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "3NsIiZR6F8B4",
    "outputId": "79498c5d-1373-479e-b07f-65e172bb01e7"
   },
   "outputs": [],
   "source": [
    "kernel = 1* RBF(length_scale=0.01, length_scale_bounds=(1e-05, 1))+0.39\n",
    "yte_pred_gs_norm = gpr_pred(X_tr, y_tr, X_te, kernel)\n",
    "yte_pred_gs = scale_back(yte_pred_gs_norm, df['residual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK3J8VS9UtkN"
   },
   "source": [
    "## 3.4. Random Forest\n",
    "\n",
    "The Random Forests are an ensemble learning method that use the \"bagging\" technique to combine many decision trees in order to improve the statistical stability of decision trees and reduce overfitting. More specifically, random forest trains *N* trees, with each tree fitted to a bootstrap sample (i.e. sample with replacement) of the original training data.\n",
    "\n",
    "### OOB error\n",
    "Each bootstrap sample leaves out about one-third of the training data, which is called the out-of-bag (OOB) data. The OOB error is the average error of a tree evaluated on the OOB data. Since for each tree, the OOB data is unseen during training, OOB error provides a convenient way to estimate generalization error. OOB error can also be used to assess the importance of each input variables by permuting each variable in turn and track the resulting increase in OOB error.\n",
    "\n",
    "### Hyperparameters\n",
    "`n_estimators`: Number of trees in the forest.\n",
    "\n",
    "`max_features`: The maximum number of variables used at each split for each tree. This item is used to avoid overfitting.\n",
    "\n",
    "`min_samples_split`: The minimum number of samples required to split and grow new branches at an internal node. The smaller the number, the higher the chance model tends to overfit.\n",
    "\n",
    "`min_samples_leaf`: The minimum number of samples required to be at a leaf node. The smaller the number, the higher the chance model tends to overfit.\n",
    "\n",
    "We will utilize the `GridSearchCV` function in the `sklearn.model_selection` package to select hyperparameters.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKlUAvfEUXA0"
   },
   "outputs": [],
   "source": [
    "# Grid search to tune hyperparameters using oob (no need to reserve a validation set). Number of trees, random subset per split, leaf size\n",
    "# Use best hyperparameters to train and test\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "# X train is input features\n",
    "# y _train is target\n",
    "\n",
    "# self define score function for Gridsearch, using random forest oob error as score/ criteria\n",
    "def oob_scorer(estimator, X, y):\n",
    "    return estimator.oob_score_\n",
    "\n",
    "# set up the hyperparameter grids. For each hyperparameter combination, train a\n",
    "# and evaluate OOB error\n",
    "param_grid = {\n",
    "    'n_estimators':np.arange(100,500,100),\n",
    "    'max_features' : [2,3],\n",
    "    'min_samples_split' : np.arange(2,32,4),\n",
    "    'min_samples_leaf' : np.arange(2,32,4),\n",
    "}\n",
    "\n",
    "def Grid_search(X_train, y_train,param_grid):\n",
    "# random forest regression\n",
    "    rfr = RandomForestRegressor(n_estimators = 500, n_jobs = -1,random_state=random_seed,oob_score=True)\n",
    "    #set up grid search\n",
    "    model_rf = GridSearchCV(estimator=rfr,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring=oob_scorer,\n",
    "                      )\n",
    "    # fit and train\n",
    "    model_rf.fit(X_train, y_train)\n",
    "\n",
    "    #print best parameters\n",
    "    model_rf.best_params_\n",
    "\n",
    "    #print the best train score (oob error)\n",
    "    model_rf.score(X_train, y_train)\n",
    "\n",
    "    return(model_rf.best_params_)\n",
    "    # predict test data\n",
    "def RFM(X_tr, y_tr,X_te, best_par):\n",
    "    rfr_2=  RandomForestRegressor(n_estimators =best_par['n_estimators'],max_features=best_par['max_features'], min_samples_leaf=best_par['max_features'],\n",
    "\n",
    "                                  min_samples_split=best_par['min_samples_split'],n_jobs = -1,random_state=random_seed,oob_score=True) # set RF model with best parameter\n",
    "    rfr_2.fit(X_tr, y_tr) # fit RF with Train data set\n",
    "    yte_pred=rfr_2.predict(X_te) # RF test\n",
    "    return(yte_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1910658,
     "status": "ok",
     "timestamp": 1666651192162,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "uFbh2md_URyd",
    "outputId": "6ec5b811-e963-493d-deb0-646dc96b73f7"
   },
   "outputs": [],
   "source": [
    "# determine best hyperparameters\n",
    "best_par = Grid_search(X_tr, y_tr,param_grid)\n",
    "print(best_par)\n",
    "# make predictions\n",
    "yte_pred_rf_norm = RFM(X_tr, y_tr,X_te, best_par)\n",
    "# scale back\n",
    "yte_pred_rf = scale_back(yte_pred_rf_norm , df['residual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD6yJGUURfoj"
   },
   "source": [
    "# 3.5. Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjGGRmDTo7Uk"
   },
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "MLP, also known as feedforward neural networks, are inspired by biological learning processes and built out of a densely interconnected set of units or neurons. A typical MLP network consists of an input layer, one or more hidden layers and an output layer. Information flows through the connections between units. Each unit computes a single output by passing the weighted sum of its inputs plus a bias term through a nonlinear activation function (e.g., sigmoid or rectifier).\n",
    "\n",
    "In this example, we build a simple MLP model for bias correction. We need to tune the model hyperparameters, including number of hidden layers, number of hidden units in each layer, type of the activation function, and learning rate.\n",
    "\n",
    "The learning rate determines the size of updates applied on the weights in each training iteration (i.e., epoch). A too low learning rate results in very long training process, while a too high learning rate may overshoot and have trouble finding the optimum.\n",
    "\n",
    "Number of hidden layers and units determine the complexity of the network; for this relatively simple example, we keep them small so the model won't overfit.\n",
    "\n",
    "Use 'early_stopping' to avoid overfitting;\n",
    "It will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least 1e-4 (By default,  can be adjust by 'tol') for 10 (n_iter_no_change) consecutive epochs. Only effective when solver=’sgd’ or ‘adam’. quoted from: (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8BktwV5-Q8P"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_MLP = {\n",
    "    'hidden_layer_sizes':[(16,2),(8,1),(16,1)],\n",
    "    'activation' : ['relu','sigmoid','tanh'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate_init' :np.arange(0.00001,0.001,0.0001),\n",
    "}\n",
    "\n",
    "\n",
    "def Grid_search_MLP(X_train, y_train,param_grid):\n",
    "# MLP\n",
    "    regr = MLPRegressor(random_state=random_seed,early_stopping=True, max_iter=500)\n",
    "    #set up grid search\n",
    "    model_mlp = GridSearchCV(estimator=regr,param_grid=param_grid, cv=5)\n",
    "\n",
    "    # fit and train\n",
    "    model_mlp.fit(X_train, y_train)\n",
    "\n",
    "    #print best parameters\n",
    "    model_mlp.best_params_\n",
    "\n",
    "    #print the best train score (oob error)\n",
    "    model_mlp.score(X_train, y_train)\n",
    "\n",
    "    return(model_mlp.best_params_)\n",
    "\n",
    "\n",
    "def mlp_pre(X_tr, y_tr,X_te, best_par):\n",
    "    regr_2 = MLPRegressor(random_state=random_seed, max_iter=500,\n",
    "                          hidden_layer_sizes=best_par['hidden_layer_sizes'],\n",
    "                          activation=best_par['activation'],\n",
    "                          solver=best_par['solver'],\n",
    "                          learning_rate_init=best_par['learning_rate_init'],\n",
    "                          early_stopping=True)\n",
    "\n",
    "    regr_2.fit(X_tr, y_tr) # fit RF with Train data set\n",
    "    yte_pred=regr_2.predict(X_te) # RF test\n",
    "    return(yte_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783275,
     "status": "ok",
     "timestamp": 1666651975434,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "xdQJN3sBUbh-",
    "outputId": "660590e7-1890-44d8-bf7b-26deca2eb569"
   },
   "outputs": [],
   "source": [
    "# determine best hyperparameters\n",
    "best_par_mlp = Grid_search_MLP(X_tr, y_tr,param_grid_MLP)\n",
    "print(best_par_mlp)\n",
    "# make predictions\n",
    "yte_pred_mlp_norm = mlp_pre(X_tr, y_tr,X_te, best_par_mlp)\n",
    "# scale back\n",
    "yte_pred_mlp = scale_back(yte_pred_mlp_norm , df['residual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMFApuxOVUPL"
   },
   "source": [
    "# 4. Performance Evaluation\n",
    "Now that we have built three machine learning models for bias correction, let's assess how well each of them performs. We will use the residual predicted by each model on the test data to update the groundwater model simulation results, calculate the remaining residual after updating, and then perform diagnostics similarly as in Section 3 to see the effectiveness of bias correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF576ysEUgQG"
   },
   "source": [
    "## 4.1. Performance metrics after bias correction\n",
    "First, let's re-calculate the performance metrics used in 2.1, now after machine learning-based bias correction. For the sample dataset, the bias correction led to PBIAS closer to zero, lower RMSE, and higher NSE and KGE for the test dataset, all suggesting better match to observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1666651975662,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "KYedhABXsVeu",
    "outputId": "2c50b338-fabd-4317-c98c-b41a00ffb313"
   },
   "outputs": [],
   "source": [
    "df_te['ml_GPR'] = df_te['MODFLOW'] + yte_pred_gs\n",
    "df_te['ml_RF'] = df_te['MODFLOW'] + yte_pred_rf\n",
    "df_te['ml_MLP'] = df_te['MODFLOW'] + yte_pred_mlp\n",
    "for ml_index in ['ml_GPR','ml_RF','ml_MLP']:#,\n",
    "    kge_per_well_te, nse_per_well_te = kge_nse(df_te, 'observation',ml_index)\n",
    "    nse_rm_inf_nan_te = nse_per_well_te[np.isfinite(nse_per_well_te)] # remove Nan and Inf values if there are\n",
    "    print('PBIAS of '+ ml_index +':', PBIAS(df_te['observation'],df_te[ml_index]))\n",
    "    print('RMSE '+ ml_index +':', RMSE(df_te['observation'],df_te[ml_index]))\n",
    "    print('NSE '+ ml_index +':', np.nanmedian(nse_rm_inf_nan_te))\n",
    "    print('KGE '+ ml_index +':', np.nanmedian(kge_per_well_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q61STTFhUoKO"
   },
   "source": [
    "## 4.2. Residual plots after bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4679,
     "status": "ok",
     "timestamp": 1666651980339,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "3_jvmhkEjjoY",
    "outputId": "cc17c728-8a52-440f-b84c-5ccaf63529b2"
   },
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=2, ncols=2, sharex='all',figsize=(10,10),dpi=500)\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_result=['MODFLOW','ml_RF','ml_GPR','ml_MLP']\n",
    "plot_legend=['Before BC','BC with RF','BC with GPR','BC with MLP']\n",
    "for n_model in range (len(model_result)):\n",
    "    axes[n_model].scatter(df_te['observation'],df_te['observation']-df_te[model_result[n_model]],\n",
    "                        marker='*',c='red' ,s=1, label=plot_legend[n_model])\n",
    "    axes[n_model].set_xlabel('Observation (ft)')\n",
    "    axes[n_model].set_ylabel('Residual (ft)')\n",
    "    axes[n_model].legend()\n",
    "    axes[n_model].set_ylim([min(df_te['residual']), max(df_te['residual'])])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C46whmyvSf_g"
   },
   "source": [
    "Next, let's plot out groundwater level time series at a few wells. The figure below compares the observed groundwater level, groundwater simulated (No BC), and after bias correction (BC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2961,
     "status": "ok",
     "timestamp": 1666651983290,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "n-_P_SfyYVO5",
    "outputId": "fbd808fb-881d-49c7-c75b-a3d9c7bd97f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdate\n",
    "random_pick = [236,251,2,13]\n",
    "print(random_pick)\n",
    "\n",
    "fig,axes = plt.subplots(nrows=2, ncols=2,figsize=(10,10),dpi=300)\n",
    "fig.autofmt_xdate(rotation=90)\n",
    "for n in range (4):\n",
    "# sort data based on its time\n",
    "    well_plt = df_te[df_te['well_no']==random_pick[n]]\n",
    "    well_plt['date_formated'] = pd.to_datetime(well_plt['date_formated'])\n",
    "    well_sorted = well_plt.sort_values(by=['date_formated'])\n",
    "\n",
    "    # plot\n",
    "    axes = axes.flatten()\n",
    "    axes[n].plot(well_sorted['date_formated'], well_sorted['observation'], '-o', c='g', label='Obs.')\n",
    "    axes[n].plot(well_sorted['date_formated'], well_sorted['MODFLOW'], '-^', c='r', label='No BC')\n",
    "    axes[n].plot(well_sorted['date_formated'], well_sorted['ml_RF'],'-*', c='b', label='BC w/ RF')\n",
    "    axes[n].plot(well_sorted['date_formated'], well_sorted['ml_MLP'],'-*', c='orange', label='BC w/ MLP')\n",
    "    axes[n].plot(well_sorted['date_formated'], well_sorted['ml_GPR'],'-*', c='grey', label='BC w/ GPR')\n",
    "    axes[n].set_title( 'Well No. ' + str(random_pick[n]))\n",
    "    axes[n].set_ylabel('(ft)')\n",
    "    axes[n].set_xlabel('Time')\n",
    "    monthFmt = mdate.DateFormatter('%Y-%m')\n",
    "    axes[n].xaxis.set_major_formatter(monthFmt)\n",
    "    axes[n].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2300,
     "status": "ok",
     "timestamp": 1666651985586,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "QDVib4jvZwK3",
    "outputId": "1d06264c-66d0-4d34-ebbf-00ad894f89ca"
   },
   "outputs": [],
   "source": [
    "df_time_average_te=df_te.groupby(df_te['well_no']).mean() # get the temporal average value of each site\n",
    "\n",
    "x_co_time_average_te=df_time_average_te['x_coordinates']\n",
    "y_co_time_average_te=df_time_average_te['y_coordinates']\n",
    "\n",
    "fig,axes=plt.subplots(nrows=2, ncols=2, sharex='all',figsize=(10,10),dpi=300)\n",
    "axes = axes.flatten()\n",
    "plot_bc=['MODFLOW','ml_RF','ml_GPR','ml_MLP']\n",
    "plot_title=['No BC','BC w/RF','BC w/GPR','BC w/MLP']\n",
    "for n in range(len(plot_bc)):#,\n",
    "    bc_index=plot_bc[n]\n",
    "    offset = mcolors.TwoSlopeNorm(vmin=-30,\n",
    "                                  vcenter=0, vmax=30)\n",
    "    im1=axes[n].scatter(x_co_time_average_te, y_co_time_average_te, c=df_time_average_te['observation']-df_time_average_te[bc_index], norm=offset,cmap='RdBu')\n",
    "    axes[n].set_title(plot_title[n])\n",
    "    axes[n].set_xlabel(\"X (mi.)\")\n",
    "    axes[n].set_ylabel(\"y (mi.)\")\n",
    "    fig.colorbar(im1, ax=axes[n],label='Residual (ft)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2416,
     "status": "ok",
     "timestamp": 1666651987992,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "EuwDoys2UP4H",
    "outputId": "940c3dcc-94c8-4438-9db3-2d61048704f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_bc=['MODFLOW','ml_RF','ml_GPR','ml_MLP']\n",
    "plot_title=['No BC','BC w/RF','BC w/GPR','BC w/MLP']\n",
    "fig, axes=plt.subplots(ncols=2,nrows=2,sharex=True, sharey=True,figsize=(10,10), dpi=300)\n",
    "axes = axes.flatten()\n",
    "lag=20\n",
    "well_plot_no_te=92\n",
    "well_acf_te=df_te[df_te['well_no']==well_plot_no_te]\n",
    "for n_model in range (len(plot_bc)):\n",
    "    plot_res=well_acf_te['observation']-well_acf_te[plot_bc[n_model]]\n",
    "    acf_plot=[ACF(plot_res,i) for i in range (lag)]\n",
    "    axes[n_model].plot(list(range(lag)), acf_plot, '*-',label=plot_title[n_model])\n",
    "    axes[n_model].set_xlabel('Lags (Months)')\n",
    "    axes[n_model].set_ylabel('Correlation coefficient')\n",
    "    axes[n_model].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1666651990722,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "a_U870pV-e9K",
    "outputId": "1f794c32-6352-4d88-aae0-d46f6fab967a"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_bc=['MODFLOW','ml_RF','ml_GPR','ml_MLP']\n",
    "plot_title=['No BC','BC w/RF','BC w/GPR','BC w/MLP']\n",
    "fig, axes=plt.subplots(ncols=2,nrows=2,sharex=True,figsize=(10,10), dpi=300)\n",
    "axes = axes.flatten()\n",
    "n_bin = 30\n",
    "\n",
    "for n_model in range (len(plot_bc)):\n",
    "    res_te=df_time_average_te['observation']-df_time_average_te[plot_bc[n_model]]\n",
    "    diff_y_te, diff_dis_te = dist(df_time_average_te['x_coordinates'].to_numpy(), df_time_average_te['y_coordinates'].to_numpy(), res_te.to_numpy())\n",
    "    x_plot_te=np.linspace(start=min(diff_dis_te), stop=max(diff_dis_te), num=n_bin)\n",
    "    d_te={'dis': diff_dis_te, 'res': diff_y_te}\n",
    "    new_df_te=pd.DataFrame(data=d_te)\n",
    "    plt_y_te=new_df_te['res'].groupby(pd.cut(new_df_te[\"dis\"], x_plot_te)).mean()\n",
    "    axes[n_model].plot(x_plot_te[:-1]/5280,plt_y_te,'s-',label=plot_title[n_model]+' residual semi-variogram')\n",
    "    axes[n_model].plot(x_plot_te[:-1]/5280,[statistics.variance(res_te) for k in range (len(x_plot_te[:-1]))], '--', label=plot_title[n_model]+' residual variance')\n",
    "    axes[n_model].set_xlabel('Distance (mi.)')\n",
    "    axes[n_model].set_ylabel(r'$\\gamma (ft^2)$')\n",
    "    axes[n_model].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3ZoSdZTUPlX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3f8LC5ZvPcS"
   },
   "source": [
    "## 4.3. Write the bias corrected model outputs back to HydroShare\n",
    "\n",
    "Finally, we will save the bias corrected results and upload to HydroShare. For more technical details, refer to the documentation of `hsclient` named `file_operation_ipyn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29202,
     "status": "ok",
     "timestamp": 1666652019921,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "YKkvGEJGILuw",
    "outputId": "48f0cecf-7d73-41da-b6ff-aeb6e59aa943"
   },
   "outputs": [],
   "source": [
    "\"'Machine learning model to bias correction all samples'\"\n",
    "pred_gs_norm = gpr_pred(X_tr, y_tr, df_X, kernel)\n",
    "pred_gs = scale_back(pred_gs_norm, df['residual'])\n",
    "\n",
    "pred_rf_norm = RFM(X_tr, y_tr,df_X, best_par)\n",
    "# scale back\n",
    "pred_rf = scale_back(pred_rf_norm , df['residual'])\n",
    "\n",
    "\n",
    "pred_mlp_norm = mlp_pre(X_tr, y_tr,df_X, best_par_mlp)\n",
    "# scale back\n",
    "pred_mlp = scale_back(pred_mlp_norm, df['residual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc3xTsJQI8tZ"
   },
   "outputs": [],
   "source": [
    "\"'Save machine learning result to dataset'\"\n",
    "df['BC w/RF']=pred_rf+df['MODFLOW']\n",
    "df['BC w/GPR']=pred_gs+df['MODFLOW']\n",
    "df['BC w/MLP']=pred_mlp+df['MODFLOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "error",
     "timestamp": 1666652020149,
     "user": {
      "displayName": "Tianfang Xu",
      "userId": "02396096612949262001"
     },
     "user_tz": 420
    },
    "id": "H4tADLMEopyX",
    "outputId": "3d700300-481e-472b-9cfe-1bf36359a8b2"
   },
   "outputs": [],
   "source": [
    "\"'Save new dataset to local'\"\n",
    "output_path='' # it is better to save it to Microsoft Excel Worksheet (.xlsx) formate. Since csv automatically do scientific notation for value has more than 12 digitial numbers.\n",
    "df.to_excel(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbCZNbL1gP4k"
   },
   "outputs": [],
   "source": [
    "\"'Upload to hydroshare'\"\n",
    "# Create the new, empty resource\n",
    "hs = HydroShare()\n",
    "hs.sign_in()\n",
    "new_resource = hs.create()\n",
    "\n",
    "# Get the HydroShare identifier for the new resource\n",
    "resIdentifier = new_resource.resource_id\n",
    "print('The HydroShare Identifier for your new resource is: ' + resIdentifier)\n",
    "\n",
    "# Construct a hyperlink for the new resource\n",
    "print('Your new resource is available at: ' +  new_resource.metadata.url)\n",
    "\n",
    "# Upload one or more files to your resource\n",
    "file_upload_path=''\n",
    "new_resource.file_upload(file_upload_path)\n",
    "\n",
    "# Print the names of the files in the resource\n",
    "print('Updated file list after adding a file: ')\n",
    "for file in new_resource.files(search_aggregations=True):\n",
    "    print(file.path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
